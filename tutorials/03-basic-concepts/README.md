# ğŸ”§ Core Concepts Guide

Understanding the fundamental concepts of the Experimentation Platform.

## ğŸ—ï¸ Architecture Overview

The platform is built around four core components:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ğŸ“ Dataset    â”‚â”€â”€â”€â–¶â”‚  ğŸƒ Execution   â”‚â”€â”€â”€â–¶â”‚  ğŸ“Š Evaluation  â”‚
â”‚                 â”‚    â”‚     Module      â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚                       â”‚
                                â–¼                       â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚ âš™ï¸ Configuration â”‚    â”‚  ğŸ’¾ Results     â”‚
                       â”‚                 â”‚    â”‚                 â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Dataset

Your **data** that drives the experiments:

### Types
- **Inline**: Data defined directly in YAML
- **File**: CSV, JSON, or other file formats  
- **Dynamic**: Generated programmatically

### Structure
```yaml
dataset:
  type: inline
  data:
    - input: "Question 1"
      expected_output: "Answer 1"
      context: "additional_info"
```

## ğŸƒ Execution Module

The **code** that processes your data:

### Requirements
- Must have a `run()` function
- Takes input data as parameters
- Returns results for evaluation

### Example
```python
def run(input_data: str, context: str = None) -> str:
    # Your processing logic here
    return processed_result
```

## ğŸ“Š Evaluators

The **metrics** that measure performance:

### Platform Evaluators
Built-in evaluators for common tasks:
- `conversation_quality` - Chat/dialogue quality
- `tool_call_accuracy` - API/tool usage correctness
- `response_relevance` - Answer relevance scoring

### Foundry Evaluators  
Custom evaluators using the foundry pattern:
- Row-by-row processing
- Flexible input/output handling
- Custom scoring logic

## âš™ï¸ Configuration

The **YAML file** that ties everything together:

### Key Sections
```yaml
name: experiment_name
description: "What this experiment does"

dataset: # Your data source
evaluators: # How to measure success  
execution: # How to run the code
output: # Where to save results
```

## ğŸ’¾ Results

The **output** of your experiments:

### What's Saved
- **Raw Results**: Individual evaluation scores
- **Aggregated Metrics**: Summary statistics
- **Execution Logs**: Detailed run information
- **Configuration**: Resolved experiment setup

### File Structure
```
data/experiments/
â””â”€â”€ experiment_name_20241215_143052/
    â”œâ”€â”€ results.json
    â”œâ”€â”€ execution.log
    â””â”€â”€ config.yaml
```

## ğŸ”„ Execution Flow

1. **Load Configuration** - Parse YAML file
2. **Prepare Dataset** - Load and validate data
3. **Initialize Evaluators** - Set up measurement tools
4. **Execute Module** - Run processing logic
5. **Evaluate Results** - Calculate metrics
6. **Save Output** - Store results and logs

## ğŸ¯ Best Practices

### Configuration
- âœ… Use descriptive names and descriptions
- âœ… Set appropriate timeouts
- âœ… Include metadata for tracking

### Modules
- âœ… Handle errors gracefully
- âœ… Return consistent data types
- âœ… Include logging for debugging

### Evaluation
- âœ… Choose appropriate evaluators
- âœ… Set realistic thresholds
- âœ… Document scoring criteria

## ğŸ”— Next Steps

Ready to build your first experiment?

- **[Simple Experiment](../04-simple-experiment/)** - Build from scratch
- **[Configuration Guide](../05-configuration/)** - Master YAML configs
- **[Evaluator Guide](../06-evaluators/)** - Understand evaluation metrics

---

**ğŸ’¡ Key Takeaway**: The platform provides a structured way to run **reproducible experiments** with **consistent evaluation** across different **execution contexts**.